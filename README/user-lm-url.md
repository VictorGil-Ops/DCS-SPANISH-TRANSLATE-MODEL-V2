# URL del Servidor de Modelo de Lenguaje

## ¬øQu√© es la URL del servidor LM?

Es la **direcci√≥n web** donde est√° ejecut√°ndose tu modelo de lenguaje (como LM Studio, Ollama, o cualquier servidor compatible con OpenAI API). El traductor se conecta a esta direcci√≥n para enviar textos y recibir traducciones.

## Formato de URL

### üåê **Estructura t√≠pica**
```
http://[direcci√≥n]:[puerto]/v1
```

### üìã **Ejemplos comunes**

#### **LM Studio (local)**
```
http://localhost:1234/v1
http://127.0.0.1:1234/v1
```

#### **Ollama (local)**
```
http://localhost:11434/v1
```

#### **Servidor remoto**
```
http://192.168.1.100:1234/v1
http://mi-servidor.local:8080/v1
```

#### **OpenAI API (oficial)**
```
https://api.openai.com/v1
```

## Configuraci√≥n paso a paso

### üöÄ **Para LM Studio**

1. **Inicia LM Studio**
2. **Carga un modelo** (ej: Llama-3-8B, Gemma-2B)
3. **Ve a la pesta√±a "Local Server"**
4. **Inicia el servidor** (Start Server)
5. **Copia la URL** que aparece (normalmente `http://localhost:1234`)
6. **P√©gala en el traductor** con `/v1` al final

### üê≥ **Para Ollama**

1. **Instala Ollama** desde ollama.ai
2. **Descarga un modelo**: `ollama pull llama3`
3. **Inicia el servidor**: `ollama serve`
4. **Usa la URL**: `http://localhost:11434/v1`

### üîó **Para servidor remoto**

1. **Obt√©n la IP** del servidor donde corre el modelo
2. **Averigua el puerto** (pregunta al administrador)
3. **Construye la URL**: `http://[IP]:[Puerto]/v1`

## ¬øC√≥mo verificar que funciona?

### ‚úÖ **Prueba de conexi√≥n**

El traductor incluye un **bot√≥n de test** que verifica:
- ‚úÖ Si la URL es accesible
- ‚úÖ Si el servidor responde
- ‚úÖ Si hay modelos disponibles
- ‚úÖ Si la API es compatible

### üîç **Prueba manual (avanzado)**

Abre un navegador y visita:
```
http://tu-servidor:puerto/v1/models
```

Deber√≠as ver una lista de modelos disponibles.

## Configuraciones especiales

### üîê **Con autenticaci√≥n (API Keys)**
Si tu servidor requiere autenticaci√≥n:
1. Configura la URL normalmente
2. En el campo "API Key", ingresa tu clave
3. El traductor la enviar√° autom√°ticamente

### üåç **Usando servicios en la nube**
- **OpenAI**: Necesitas una cuenta y API Key
- **Hugging Face**: Puedes usar endpoints de modelos hospedados
- **Replicate**: Para modelos especializados

### üè† **Red local (LAN)**
Para usar un servidor en otra computadora:
1. Encuentra la IP de la computadora servidor
2. Aseg√∫rate de que el firewall permita la conexi√≥n
3. Usa `http://[IP-de-la-computadora]:puerto/v1`

## Resoluci√≥n de problemas comunes

### ‚ùå **"No se puede conectar"**
- **Verifica que el servidor est√© funcionando**
- **Revisa la URL** (puerto correcto, http vs https)
- **Desactiva firewall** temporalmente para probar

### ‚ùå **"Timeout de conexi√≥n"**
- **El modelo es muy lento** para tu hardware
- **Aumenta el timeout** en configuraciones avanzadas
- **Usa un modelo m√°s peque√±o**

### ‚ùå **"API Key inv√°lida"**
- **Verifica la clave** si usas servicios de pago
- **Para servidores locales**, deja el campo vac√≠o

### ‚ùå **"No hay modelos disponibles"**
- **Aseg√∫rate de haber cargado un modelo** en LM Studio/Ollama
- **Verifica que el modelo est√© activo**

## Recomendaciones de rendimiento

### ‚ö° **Para traducci√≥n r√°pida**
- Usa **modelos peque√±os** (2B-7B par√°metros)
- **Servidor local** en la misma computadora
- **SSD r√°pido** para cargar modelos

### üéØ **Para m√°xima calidad**
- Usa **modelos grandes** (13B+ par√°metros)
- **GPU potente** si est√° disponible
- **Servidor dedicado** para no competir por recursos

### üí∞ **Para uso espor√°dico**
- **OpenAI API** puede ser m√°s econ√≥mico que hardware dedicado
- **Servicios en la nube** para traducciones ocasionales
# Modelo de Lenguaje

## ¬øQu√© es el modelo de lenguaje?

Es la **inteligencia artificial** espec√≠fica que realizar√° las traducciones. Cada modelo tiene diferentes capacidades, velocidad, calidad y requisitos de hardware.

## Tipos de modelos disponibles

### üéØ **Modelos Peque√±os (2B-3B par√°metros)**

#### **Gemma 2B**
- **Tama√±o**: ~1.4 GB
- **RAM requerida**: 4-6 GB
- **Velocidad**: Muy r√°pida
- **Calidad**: B√°sica, ideal para pruebas
- **Ideal para**: PCs b√°sicos, traducciones de prueba

#### **Llama 3.2 3B**
- **Tama√±o**: ~1.9 GB
- **RAM requerida**: 6-8 GB
- **Velocidad**: R√°pida
- **Calidad**: Buena para textos simples
- **Ideal para**: Equipos modestos, traducciones frecuentes

### ‚öñÔ∏è **Modelos Medianos (7B-13B par√°metros)**

#### **Llama 3.1 8B**
- **Tama√±o**: ~4.7 GB
- **RAM requerida**: 8-12 GB
- **Velocidad**: Moderada
- **Calidad**: Muy buena, equilibrada
- **Ideal para**: La mayor√≠a de usuarios

#### **Qwen 2.5 7B**
- **Tama√±o**: ~4.4 GB
- **RAM requerida**: 8-10 GB
- **Velocidad**: Buena
- **Calidad**: Excelente para textos t√©cnicos
- **Ideal para**: Traducciones t√©cnicas de aviaci√≥n

#### **Mistral 7B**
- **Tama√±o**: ~4.1 GB
- **RAM requerida**: 8-10 GB
- **Velocidad**: R√°pida
- **Calidad**: Buena, muy consistente
- **Ideal para**: Uso productivo regular

### üî• **Modelos Grandes (27B+ par√°metros)**

#### **Qwen 2.5 27B**
- **Tama√±o**: ~15.8 GB
- **RAM requerida**: 20-32 GB
- **Velocidad**: Lenta
- **Calidad**: Excelente, muy natural
- **Ideal para**: PCs high-end, traducciones finales

#### **Llama 3.1 70B**
- **Tama√±o**: ~40 GB
- **RAM requerida**: 48-64 GB
- **Velocidad**: Muy lenta
- **Calidad**: Excepcional, nivel profesional
- **Ideal para**: Servidores dedicados, m√°xima calidad

## ¬øC√≥mo elegir el modelo correcto?

### üíª **Seg√∫n tu hardware**

#### **PC B√°sico** (8GB RAM, sin GPU dedicada)
```
Recomendado: Gemma 2B, Llama 3.2 3B
Preset: Ligero
Tiempo esperado: 5-15 min por campa√±a mediana
```

#### **PC Medio** (16GB RAM, GPU dedicada)
```
Recomendado: Llama 3.1 8B, Qwen 2.5 7B
Preset: Balanceado
Tiempo esperado: 10-30 min por campa√±a mediana
```

#### **PC Potente** (32GB+ RAM, GPU high-end)
```
Recomendado: Qwen 2.5 27B, Llama 3.1 70B
Preset: Pesado
Tiempo esperado: 30-120 min por campa√±a mediana
```

### üéØ **Seg√∫n el tipo de contenido**

#### **Textos simples** (mensajes b√°sicos, nombres)
- **Cualquier modelo** es suficiente
- **Prioriza velocidad**: Gemma 2B, Llama 3.2 3B

#### **Textos t√©cnicos** (procedimientos, briefings)
- **Modelos especializados**: Qwen 2.5 7B/27B
- **Buena comprensi√≥n**: Llama 3.1 8B

#### **Narrativa compleja** (historias, di√°logos)
- **Modelos grandes**: Qwen 2.5 27B, Llama 3.1 70B
- **Naturalidad importante**: Prioriza calidad sobre velocidad

### üí∞ **Seg√∫n costes (APIs de pago)**

#### **OpenAI API** (GPT-4, GPT-3.5)
- **GPT-4**: Excelente calidad, costoso
- **GPT-3.5**: Buena calidad, m√°s econ√≥mico
- **Ideal para**: Uso espor√°dico, traducci√≥n de alta calidad

#### **Anthropic Claude**
- **Claude 3**: Muy buena comprensi√≥n contextual
- **Ideal para**: Textos largos y complejos

## Configuraci√≥n seg√∫n el modelo

### üõ†Ô∏è **LM Studio (local)**

#### **Configuraci√≥n t√≠pica:**
```
URL: http://localhost:1234/v1
Modelo: [seleccionar desde interfaz LM Studio]
Timeout: 60-120 segundos seg√∫n modelo
```

#### **Ventajas:**
- ‚úÖ Sin coste por uso
- ‚úÖ Privacidad total
- ‚úÖ Sin l√≠mites de uso

#### **Desventajas:**
- ‚ö†Ô∏è Requiere hardware potente
- ‚ö†Ô∏è Consume recursos locales

### ‚òÅÔ∏è **Servicios en la nube**

#### **OpenAI API:**
```
URL: https://api.openai.com/v1
Modelo: gpt-4 o gpt-3.5-turbo
API Key: [tu clave]
```

#### **Ventajas:**
- ‚úÖ Sin requisitos de hardware
- ‚úÖ Siempre disponible
- ‚úÖ Calidad consistente

#### **Desventajas:**
- üí∞ Coste por uso
- üåê Requiere conexi√≥n a internet
- üîí Datos enviados externamente

## Optimizaci√≥n por modelo

### ‚ö° **Para modelos peque√±os (2B-7B):**
- **Batch size grande**: 20-50 textos por lote
- **Timeout corto**: 30-60 segundos
- **Cache activado**: Reutiliza traducciones

### ‚öñÔ∏è **Para modelos medianos (8B-13B):**
- **Batch size medio**: 10-20 textos por lote
- **Timeout medio**: 60-90 segundos
- **Balance cache**: Seg√∫n preferencia

### üî• **Para modelos grandes (27B+):**
- **Batch size peque√±o**: 5-10 textos por lote
- **Timeout alto**: 90-180 segundos
- **Cache selectivo**: Solo para textos importantes

## Resoluci√≥n de problemas comunes

### ‚ùå **"Modelo no disponible"**
- **Verifica** que el modelo est√© cargado en LM Studio
- **Confirma** que el servidor LM est√© funcionando
- **Prueba** con un modelo diferente

### ‚ùå **"Respuestas muy lentas"**
- **Cambia a un modelo m√°s peque√±o**
- **Aumenta el timeout**
- **Reduce el batch size**

### ‚ùå **"Calidad de traducci√≥n pobre"**
- **Prueba con un modelo m√°s grande**
- **Ajusta los prompts de traducci√≥n**
- **Verifica la configuraci√≥n del modelo**

### ‚ùå **"Se queda sin memoria"**
- **Usa un modelo m√°s peque√±o**
- **Cierra otros programas**
- **Reduce el batch size**

## Recomendaciones por experiencia

### üë∂ **Principiantes:**
1. **Comienza** con Llama 3.1 8B (buen equilibrio)
2. **Usa Preset Balanceado** por defecto
3. **Prueba una campa√±a peque√±a** primero

### üë®‚Äçüíº **Usuarios regulares:**
1. **Qwen 2.5 7B** para textos t√©cnicos
2. **Llama 3.1 8B** para uso general
3. **Personaliza presets** seg√∫n necesidades

### üë®‚Äçüíª **Usuarios avanzados:**
1. **M√∫ltiples modelos** para diferentes tipos de contenido
2. **Configuraciones espec√≠ficas** por campa√±a
3. **Combinaci√≥n local + nube** seg√∫n el proyecto